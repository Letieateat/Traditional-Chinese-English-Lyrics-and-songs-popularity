# -*- coding: utf-8 -*-
"""01Lyrics & Popularity_Traditional Chinese Lyrics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S3rRl-h0ydH3i0aaeWZwvnhm025yljLE

##Lyrics & Popularity (Traditional Chinese)
1. Use KKBOX Crawler to get lyrics for Traditional Chinese songs
---
"""

import csv
import json
import requests
import time #for song_timestamp and song_date
from bs4 import BeautifulSoup #for parsing HTML

# KKBOX crawler

# Top 100 Popular Traditional Chinese songs in 2023 on KKBOX
url = "https://kma.kkbox.com/charts/api/v1/yearly?category=297&lang=tc&limit=100&terr=tw&type=newrelease&year=2023"

# Get song data
response = requests.get(url)
data = json.loads(response.text)
song_list = data["data"]["charts"]["newrelease"]

# Define a function to convert original rankings (from 1 to 100) into numbers ranging from 0 to 1
def convert_ranking(original_rank):
  return (101 - original_rank) / 100

with open('TW_songs.csv', 'w', newline='', encoding="utf-8") as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(["rankings", "song_name", "artist_name", "release_date", "song_url", "lyrics"])
    for song in song_list:
        song_rank = song["rankings"]["this_period"]
        song_name = song["song_name"]
        song_url = song["song_url"]
        song_artist = song["artist_name"]
        song_timestamp = int(song["release_date"])
        song_date = time.strftime("%Y-%m-%d", time.localtime(song_timestamp))

        # Fetch lyrics from the song URL
        try:
          song_response = requests.get(song_url)
          # Handle with invalid url
        except requests.exceptions.MissingSchema:
          print(f"Invalid URL encountered: {song_url}")
        except requests.exceptions.RequestException as e:
          print(f"Request failed: {e}")
        soup = BeautifulSoup(song_response.text, "html.parser")
        lyric_div = soup.find("div", class_="lyrics")
        lyric = lyric_div.text if lyric_div else "Lyrics not found"

        # Write song information and lyrics to CSV file
        rankings = convert_ranking(song_rank) #convert original rankings (from 1 to 100) into numbers ranging from 0 to 1
        writer.writerow([rankings, song_name, song_artist, song_date, song_url, lyric])

"""2. Clean the data, tokenization, and visualization"""

!pip install git+https://github.com/APCLab/jieba-tw.git #for tokenization in Traditional Chinese
!pip install TCSP #for stopwords list in Traditional Chinese
!pip install umap-learn #for dimension reduction

import pandas as pd
import matplotlib.pyplot as plt #for visualization
import jieba #for tokenization in Traditional Chinese
import re
from TCSP import read_stopwords_list #for stopwords list in Traditional Chinese
from sklearn.feature_extraction.text import CountVectorizer #convert token frequency into numerical matrix
from umap import UMAP #for dimension reduction
from scipy.stats import pearsonr #to calculate the correlation

#define a function to remove the introduction of composer and producer in the lyrics
pattern = r'(作詞：|作曲：|編曲：).*'
def clean_lyrics(text):
    cleaned_text = re.sub(pattern, '', text)
    return cleaned_text.strip()

# Read data
df = pd.read_csv('https://raw.githubusercontent.com/Letieateat/Traditional-Chinese-English-Lyrics-and-songs-popularity/main/song-data/TW_songs.csv')

# Read stopwords list from TCSP package
stopwords = read_stopwords_list()

# Tokenize the lyrics and remove stop words
df['tokenized_lyrics'] = df['lyrics'].apply(lambda x: ' '.join([word for word in jieba.cut(clean_lyrics(x)) if word not in stopwords]))

# Convert the frequency of tokenized lyrics into numerical matrix
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['tokenized_lyrics'])

# Perform UMAP to reduce dimensionality
umap = UMAP(n_components=2, random_state=42)
X_umap = umap.fit_transform(X.toarray())

# Calculate the correlation between token frequency and rankings (popularity)
correlations = []
for i in range(X.shape[1]):
    correlation, _ = pearsonr(X[:, i].toarray().ravel(), df['rankings'])
    correlations.append(correlation)

# Calculate the correlation between UMAP components and rankings (popularity)
correlation_x, _ = pearsonr(X_umap[:, 0], df['rankings'])
correlation_y, _ = pearsonr(X_umap[:, 1], df['rankings'])

# Plot tokens in reduced space along with their correlation values
plt.figure(figsize=(10, 8))
plt.scatter(X_umap[:, 0], X_umap[:, 1], c=df['rankings'], cmap='GnBu', alpha=0.7)
# Add annotations for each point
for i, txt in enumerate(df['tokenized_lyrics']):
    plt.annotate(vectorizer.get_feature_names_out()[i], (X_umap[i, 0], X_umap[i, 1]), fontsize=8)

plt.colorbar(label='Popularity')
plt.xlabel('UMAP Component 1 (Correlation: {:.2f})'.format(correlation_x))
plt.ylabel('UMAP Component 2 (Correlation: {:.2f})'.format(correlation_y))
plt.title('Traditional Chinese Tokens in Reduced Space with Correlation to Popularity')
plt.show()